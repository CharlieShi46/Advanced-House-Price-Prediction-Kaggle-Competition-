{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:22.745970Z","iopub.execute_input":"2025-10-30T08:41:22.746340Z","iopub.status.idle":"2025-10-30T08:41:22.755541Z","shell.execute_reply.started":"2025-10-30T08:41:22.746317Z","shell.execute_reply":"2025-10-30T08:41:22.754559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:22.757326Z","iopub.execute_input":"2025-10-30T08:41:22.757873Z","iopub.status.idle":"2025-10-30T08:41:22.774767Z","shell.execute_reply.started":"2025-10-30T08:41:22.757850Z","shell.execute_reply":"2025-10-30T08:41:22.773516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BASE = \"/kaggle/input/house-prices-advanced-regression-techniques\"\n\ntrain = pd.read_csv(f\"{BASE}/train.csv\")\ntest  = pd.read_csv(f\"{BASE}/test.csv\")\n\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape:\", test.shape)\n\ntrain.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:22.775712Z","iopub.execute_input":"2025-10-30T08:41:22.775991Z","iopub.status.idle":"2025-10-30T08:41:22.840522Z","shell.execute_reply.started":"2025-10-30T08:41:22.775970Z","shell.execute_reply":"2025-10-30T08:41:22.839673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1.2 - 缺失值统计\nmissing = train.isnull().sum()\nmissing = missing[missing > 0].sort_values(ascending=False)\n\nmissing_df = pd.DataFrame({\n    \"Missing Count\": missing,\n    \"Missing %\": 100 * missing / len(train)\n})\n\nmissing_df.head(30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:22.841413Z","iopub.execute_input":"2025-10-30T08:41:22.841711Z","iopub.status.idle":"2025-10-30T08:41:22.859809Z","shell.execute_reply.started":"2025-10-30T08:41:22.841691Z","shell.execute_reply":"2025-10-30T08:41:22.858787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1.3 - 数值列的分布与偏度\nnum_features = train.select_dtypes(include=[np.number]).columns\ntrain[num_features].describe().T.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:22.862051Z","iopub.execute_input":"2025-10-30T08:41:22.862355Z","iopub.status.idle":"2025-10-30T08:41:22.937115Z","shell.execute_reply.started":"2025-10-30T08:41:22.862332Z","shell.execute_reply":"2025-10-30T08:41:22.936140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1.4 - SalePrice 分布\nplt.figure(figsize=(8,4))\nsns.histplot(train[\"SalePrice\"], kde=True, color=\"skyblue\")\nplt.title(\"Distribution of SalePrice\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:22.938043Z","iopub.execute_input":"2025-10-30T08:41:22.938343Z","iopub.status.idle":"2025-10-30T08:41:23.252446Z","shell.execute_reply.started":"2025-10-30T08:41:22.938317Z","shell.execute_reply":"2025-10-30T08:41:23.251466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================\n# Step 1: Smart Missing Value Imputation\n# =====================================\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\n\n# 1️⃣ 删除缺失率过高列\ncols_to_drop = [\"PoolQC\", \"MiscFeature\", \"Alley\"]\ntrain = train.drop(columns=cols_to_drop)\ntest = test.drop(columns=cols_to_drop)\n\n# 2️⃣ “无该设施” → 填 None\nnone_cols = [\n    \"Fence\", \"MasVnrType\", \"FireplaceQu\", \"GarageType\", \n    \"GarageFinish\", \"GarageQual\", \"GarageCond\", \n    \"BsmtFinType1\", \"BsmtFinType2\", \"BsmtExposure\", \n    \"BsmtCond\", \"BsmtQual\"\n]\nfor col in none_cols:\n    train[col] = train[col].fillna(\"None\")\n    test[col]  = test[col].fillna(\"None\")\n\n# 3️⃣ 社区分组中位数填补 LotFrontage\nfor df in [train, test]:\n    df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n        lambda x: x.fillna(x.median())\n    )\n\n# 4️⃣ GarageYrBlt 缺失 → YearBuilt\ntrain[\"GarageYrBlt\"] = train[\"GarageYrBlt\"].fillna(train[\"YearBuilt\"])\ntest[\"GarageYrBlt\"]  = test[\"GarageYrBlt\"].fillna(test[\"YearBuilt\"])\n\n# 5️⃣ Electrical → 众数\ntrain[\"Electrical\"] = train[\"Electrical\"].fillna(train[\"Electrical\"].mode()[0])\n\n# 6️⃣ 用回归模型预测 MasVnrArea\nfeatures = [\"MasVnrType\", \"OverallQual\", \"YearBuilt\", \"TotalBsmtSF\", \"1stFlrSF\"]\ntrain_temp = pd.get_dummies(train[features + [\"MasVnrArea\"]], drop_first=True)\ntest_temp  = pd.get_dummies(test[features + [\"MasVnrArea\"]], drop_first=True)\n\n# 对齐列！！！！（很重要的点）\ntrain_temp, test_temp = train_temp.align(test_temp, join=\"left\", axis=1, fill_value=0)\n\n# 训练随机森林用于预测 MasVnrArea\nnotnull_mask = train_temp[\"MasVnrArea\"].notnull()\nrf = RandomForestRegressor(n_estimators=200, random_state=42)\nrf.fit(train_temp.loc[notnull_mask].drop(columns=[\"MasVnrArea\"]),\n       train_temp.loc[notnull_mask, \"MasVnrArea\"])\n\n# 预测 MasVnrArea 缺失部分\nnull_mask = train_temp[\"MasVnrArea\"].isnull()\ntrain.loc[null_mask, \"MasVnrArea\"] = rf.predict(\n    train_temp.loc[null_mask].drop(columns=[\"MasVnrArea\"])\n)\n# 对 test 同样填补\nnull_mask_test = test_temp[\"MasVnrArea\"].isnull()\ntest.loc[null_mask_test, \"MasVnrArea\"] = rf.predict(\n    test_temp.loc[null_mask_test].drop(columns=[\"MasVnrArea\"])\n)\n\n# 7️⃣ KNNImputer 全局补数值列\nnum_cols = [c for c in train.select_dtypes(include=[np.number]).columns if c != \"SalePrice\"]\nimputer = KNNImputer(n_neighbors=5)\ntrain[num_cols] = imputer.fit_transform(train[num_cols])\ntest[num_cols]  = imputer.transform(test[num_cols])\n\nprint(\"✅ Smart Missing Value Imputation complete!\")\n\n# 验证\nmissing_after = train.isnull().sum()\nmissing_after = missing_after[missing_after > 0].sort_values(ascending=False)\nprint(\"仍存在缺失列:\")\nprint(missing_after if len(missing_after) > 0 else \"无缺失值 ✅\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:23.253499Z","iopub.execute_input":"2025-10-30T08:41:23.253768Z","iopub.status.idle":"2025-10-30T08:41:24.116509Z","shell.execute_reply.started":"2025-10-30T08:41:23.253749Z","shell.execute_reply":"2025-10-30T08:41:24.115371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================================\n# Step 2: Feature Engineering\n# ========================================\n\n# ------- 1️⃣ 新建衍生特征 -------\ntrain[\"TotalSF\"] = train[\"TotalBsmtSF\"] + train[\"1stFlrSF\"] + train[\"2ndFlrSF\"]\ntest[\"TotalSF\"]  = test[\"TotalBsmtSF\"]  + test[\"1stFlrSF\"]  + test[\"2ndFlrSF\"]\n\ntrain[\"Age\"] = train[\"YrSold\"] - train[\"YearBuilt\"]\ntest[\"Age\"]  = test[\"YrSold\"]  - test[\"YearBuilt\"]\n\ntrain[\"RemodAge\"] = train[\"YrSold\"] - train[\"YearRemodAdd\"]\ntest[\"RemodAge\"]  = test[\"YrSold\"]  - test[\"YearRemodAdd\"]\n\n# 车库平均面积（避免除以0）！！！很重要\ntrain[\"GarageAreaPerCar\"] = train[\"GarageArea\"] / (train[\"GarageCars\"] + 1e-3)\ntest[\"GarageAreaPerCar\"]  = test[\"GarageArea\"]  / (test[\"GarageCars\"] + 1e-3)\n\n# ------- 2️⃣ 等级型文字转数值 -------\nqual_map = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"None\": 0}\n\nquality_cols = [\n    \"ExterQual\", \"ExterCond\", \"HeatingQC\", \"KitchenQual\",\n    \"FireplaceQu\", \"GarageQual\", \"GarageCond\",\n    \"BsmtQual\", \"BsmtCond\"\n]\n\n# #！！！很重要 也就是说：\n# \t•\t有的缺失值是 NaN（真正的空值）\n# \t•\t有的缺失值是 字符串 \"None\"\n\n# map(qual_map) 只会映射 \"None\" → 0\n# 但如果某个格子原本是 NaN，那它不会被映射成功，结果还是 NaN。\n# 这里保证无论是 “None” 还是 NaN → 都统一映射为 0\nfor col in quality_cols:\n    if col in train.columns:  # 防止有的列已被删除\n        train[col] = train[col].map(qual_map).fillna(0)\n        test[col]  = test[col].map(qual_map).fillna(0)\n\n# ------- 3️⃣ 检查结果 -------\nnew_features = [\"TotalSF\", \"Age\", \"RemodAge\", \"GarageAreaPerCar\"] + quality_cols\nprint(\"✅ 特征工程完成！新增列：\")\nprint(new_features)\n\ntrain[new_features].head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:24.117687Z","iopub.execute_input":"2025-10-30T08:41:24.118000Z","iopub.status.idle":"2025-10-30T08:41:24.161652Z","shell.execute_reply.started":"2025-10-30T08:41:24.117975Z","shell.execute_reply":"2025-10-30T08:41:24.160877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================\n# Step 3: Model Comparison (Ridge / Lasso / ElasticNet)\n# =====================================\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\n\n# 目标变量 log1p 转换（RMSLE 对应）\ny = np.log1p(train[\"SalePrice\"])\nX = train.drop(columns=[\"SalePrice\"])\n\n# 数值/类别列划分\nnum_cols = X.select_dtypes(include=[np.number]).columns.tolist()\ncat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n\n# --- 预处理模块：数值和类别分别处理 ---\nnumeric_tf = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\n\ncategorical_tf = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n])\n\npreprocess = ColumnTransformer([\n    (\"num\", numeric_tf, num_cols),\n    (\"cat\", categorical_tf, cat_cols)\n])\n\n# --- 模型定义 ---\nmodels = {\n    \"Ridge\": Ridge(alpha=10.0, random_state=42),\n    \"Lasso\": Lasso(alpha=0.001, random_state=42, max_iter=10000),\n    \"ElasticNet\": ElasticNet(alpha=0.001, l1_ratio=0.5, random_state=42, max_iter=10000)\n}\n\n# --- 交叉验证设置 ---\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# --- 对比三种模型 ---\nresults = []\nfor name, model in models.items():\n    pipe = Pipeline([\n        (\"prep\", preprocess),\n        (\"model\", model)\n    ])\n    scores = cross_val_score(\n        pipe, X, y,\n        scoring=\"neg_root_mean_squared_error\",\n        cv=cv,\n        n_jobs=-1\n    )\n    results.append({\n        \"Model\": name,\n        \"Mean_RMSLE\": -scores.mean(),\n        \"Std\": scores.std()\n    })\n\n# --- 输出结果 ---\nresults_df = pd.DataFrame(results).sort_values(\"Mean_RMSLE\")\nprint(\"✅ Model comparison complete!\")\nresults_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:24.162589Z","iopub.execute_input":"2025-10-30T08:41:24.162856Z","iopub.status.idle":"2025-10-30T08:41:25.413636Z","shell.execute_reply.started":"2025-10-30T08:41:24.162832Z","shell.execute_reply":"2025-10-30T08:41:25.412836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # =====================================\n# # Step 4: XGBoost Hyperparameter Tuning\n# # =====================================\n# from xgboost import XGBRegressor\n# from sklearn.model_selection import RandomizedSearchCV, KFold\n# from sklearn.pipeline import Pipeline\n# from scipy.stats import uniform, randint\n\n# # 1️⃣ 定义模型\n# xgb_model = XGBRegressor(\n#     objective=\"reg:squarederror\",\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# # 2️⃣ 定义参数搜索空间\n# param_dist = {\n#     \"model__n_estimators\": randint(400, 1500),\n#     \"model__learning_rate\": uniform(0.01, 0.09),\n#     \"model__max_depth\": randint(3, 8),\n#     \"model__min_child_weight\": randint(1, 8),\n#     \"model__subsample\": uniform(0.6, 0.4),\n#     \"model__colsample_bytree\": uniform(0.6, 0.4),\n#     \"model__reg_alpha\": uniform(0, 0.5),\n#     \"model__reg_lambda\": uniform(0.5, 2.0)\n# }\n\n# # 3️⃣ 构建 Pipeline\n# pipe_xgb = Pipeline([\n#     (\"prep\", preprocess),\n#     (\"model\", xgb_model)\n# ])\n\n# # 4️⃣ 交叉验证方案\n# cv = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# # 5️⃣ 随机搜索（每次尝试 30 组参数）\n# random_search = RandomizedSearchCV(\n#     estimator=pipe_xgb,\n#     param_distributions=param_dist,\n#     n_iter=30,                     # 搜索 30 次，兼顾精度与时间\n#     scoring=\"neg_root_mean_squared_error\",\n#     cv=cv,\n#     verbose=2,\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# # 6️⃣ 开始搜索\n# random_search.fit(X, y)\n\n# # 7️⃣ 输出最佳结果\n# print(\"✅ Best CV RMSLE-like score:\", -random_search.best_score_)\n# print(\"Best Parameters:\")\n# for k, v in random_search.best_params_.items():\n#     print(f\"   {k}: {v}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:25.416147Z","iopub.execute_input":"2025-10-30T08:41:25.416689Z","iopub.status.idle":"2025-10-30T08:41:25.421605Z","shell.execute_reply.started":"2025-10-30T08:41:25.416666Z","shell.execute_reply":"2025-10-30T08:41:25.420603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: (Optional) Hyperparameter Tuning - Skipped in Final Version\n# Best CV RMSLE: 0.1259\n# Best Parameters:\n#   n_estimators = 512\n#   learning_rate = 0.0356\n#   max_depth = 3\n#   min_child_weight = 3\n#   subsample = 0.613\n#   colsample_bytree = 0.720\n#   reg_alpha = 0.488\n#   reg_lambda = 1.322","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:25.422697Z","iopub.execute_input":"2025-10-30T08:41:25.423448Z","iopub.status.idle":"2025-10-30T08:41:25.450747Z","shell.execute_reply.started":"2025-10-30T08:41:25.423416Z","shell.execute_reply":"2025-10-30T08:41:25.449692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================\n# Step 5: Model Ensemble (Averaging + Stacking)\n# =====================================\nfrom sklearn.linear_model import Ridge, ElasticNet, LassoCV\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport pandas as pd\n\n# ---- 1️⃣ 定义三个基础模型 ----\nridge = Ridge(alpha=10.0, random_state=42)\nelastic = ElasticNet(alpha=0.001, l1_ratio=0.5, random_state=42, max_iter=10000)\nxgb_best = XGBRegressor(\n    n_estimators=512,\n    learning_rate=0.0356,\n    max_depth=3,\n    min_child_weight=3,\n    subsample=0.613,\n    colsample_bytree=0.720,\n    reg_alpha=0.488,\n    reg_lambda=1.322,\n    objective=\"reg:squarederror\",\n    random_state=42,\n    n_jobs=-1\n)\n\n# ---- 2️⃣ 准备预处理和数据 ----\nfrom sklearn.pipeline import Pipeline\n\nmodels = {\n    \"ridge\": ridge,\n    \"elastic\": elastic,\n    \"xgb\": xgb_best\n}\n\npred_train = pd.DataFrame(index=X.index)\npred_test = pd.DataFrame(index=test.index)\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# ---- 3️⃣ 5 折交叉预测，用于融合 ----\nfor name, model in models.items():\n    pipe = Pipeline([\n        (\"prep\", preprocess),\n        (\"model\", model)\n    ])\n    oof_pred = np.zeros(len(X))   # Out-Of-Fold prediction\n    test_pred = np.zeros(len(test))\n\n    for train_idx, val_idx in cv.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        pipe.fit(X_train, y_train)\n        oof_pred[val_idx] = pipe.predict(X_val)\n        test_pred += pipe.predict(test) / cv.n_splits\n\n    pred_train[name] = oof_pred\n    pred_test[name] = test_pred\n    print(f\"✅ {name} done\")\n\n# ---- 4️⃣ 简单加权平均 ----\n# 权重可以调整，例如 0.2, 0.3, 0.5\nweights = {\"ridge\": 0.2, \"elastic\": 0.3, \"xgb\": 0.5}\npred_train[\"blend\"] = sum(pred_train[k] * w for k, w in weights.items())\npred_test[\"blend\"] = sum(pred_test[k] * w for k, w in weights.items())\n\nprint(\"✅ Blending complete!\")\n\n\n# ---- 5️⃣ 输出 submission ----\ny_pred_final = np.expm1(pred_test[\"blend\"])  # 还原 log\nsubmission = pd.DataFrame({\n    \"Id\": test[\"Id\"],\n    \"SalePrice\": y_pred_final\n})\nsubmission[\"Id\"] = submission[\"Id\"].astype(int)\nsubmission.to_csv(\"submission_ensemble.csv\", index=False)\nprint(\"✅ submission_ensemble.csv generated successfully!\")\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:25.451672Z","iopub.execute_input":"2025-10-30T08:41:25.451983Z","iopub.status.idle":"2025-10-30T08:41:30.989209Z","shell.execute_reply.started":"2025-10-30T08:41:25.451962Z","shell.execute_reply":"2025-10-30T08:41:30.988235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # =====================================\n# # Step 6: Model Ensemble (Stacking with Meta-Model)\n# # =====================================\n\n# from sklearn.linear_model import Ridge, ElasticNet, LassoCV, LinearRegression\n# from xgboost import XGBRegressor\n# from sklearn.model_selection import KFold\n# from sklearn.pipeline import Pipeline\n# import numpy as np\n# import pandas as pd\n\n# # ---- 1️⃣ 定义基础模型 ----\n# ridge = Ridge(alpha=10.0, random_state=42)\n# elastic = ElasticNet(alpha=0.001, l1_ratio=0.5, random_state=42, max_iter=10000)\n# xgb_best = XGBRegressor(\n#     n_estimators=512,\n#     learning_rate=0.0356,\n#     max_depth=3,\n#     min_child_weight=3,\n#     subsample=0.613,\n#     colsample_bytree=0.720,\n#     reg_alpha=0.488,\n#     reg_lambda=1.322,\n#     objective=\"reg:squarederror\",\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# models = {\n#     \"ridge\": ridge,\n#     \"elastic\": elastic,\n#     \"xgb\": xgb_best\n# }\n\n# pred_train = pd.DataFrame(index=X.index)\n# pred_test = pd.DataFrame(index=test.index)\n\n# cv = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# # ---- 2️⃣ 5 折交叉预测，生成 meta 模型输入 ----\n# for name, model in models.items():\n#     pipe = Pipeline([\n#         (\"prep\", preprocess),\n#         (\"model\", model)\n#     ])\n#     oof_pred = np.zeros(len(X))   # Out-Of-Fold 预测\n#     test_pred = np.zeros(len(test))\n\n#     for train_idx, val_idx in cv.split(X, y):\n#         X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n#         y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n#         pipe.fit(X_train, y_train)\n#         oof_pred[val_idx] = pipe.predict(X_val)\n#         test_pred += pipe.predict(test) / cv.n_splits\n\n#     pred_train[name] = oof_pred\n#     pred_test[name] = test_pred\n#     print(f\"✅ Base model {name} done\")\n\n# # ---- 3️⃣ 二层模型：用 OOF 结果训练 meta 模型 ----\n# from sklearn.ensemble import GradientBoostingRegressor\n# meta_model = GradientBoostingRegressor(random_state=42)\n# meta_model.fit(pred_train, y)\n\n# # ---- 4️⃣ 预测 ----\n# pred_train[\"stacked\"] = meta_model.predict(pred_train)\n# pred_test[\"stacked\"] = meta_model.predict(pred_test)\n\n# # ---- 5️⃣ 输出最终结果 ----\n# y_pred_final = np.expm1(pred_test[\"stacked\"])  # 还原 log1p\n# submission = pd.DataFrame({\n#     \"Id\": test[\"Id\"].astype(int),\n#     \"SalePrice\": y_pred_final\n# })\n# submission.to_csv(\"submission_stacking.csv\", index=False)\n# print(\"✅ submission_stacking.csv generated successfully!\")\n# submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:30.990079Z","iopub.execute_input":"2025-10-30T08:41:30.990368Z","iopub.status.idle":"2025-10-30T08:41:30.995936Z","shell.execute_reply.started":"2025-10-30T08:41:30.990349Z","shell.execute_reply":"2025-10-30T08:41:30.994908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # 取出 base model 列（不包括 'stacked'）\n# base_cols = [c for c in pred_train.columns if c != \"stacked\"]\n\n# # 对齐维度\n# importances = meta_model.feature_importances_\n\n# # 构建 DataFrame\n# importance_df = pd.DataFrame({\n#     \"Base_Model\": base_cols,\n#     \"Importance\": importances\n# }).sort_values(\"Importance\", ascending=False)\n\n# # 打印结果\n# print(importance_df)\n\n# # 可视化\n# plt.bar(importance_df[\"Base_Model\"], importance_df[\"Importance\"])\n# plt.title(\"Meta Model Feature Importance (GradientBoosting)\")\n# plt.ylabel(\"Importance\")\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T08:41:30.996971Z","iopub.execute_input":"2025-10-30T08:41:30.997272Z","iopub.status.idle":"2025-10-30T08:41:31.016301Z","shell.execute_reply.started":"2025-10-30T08:41:30.997247Z","shell.execute_reply":"2025-10-30T08:41:31.015338Z"}},"outputs":[],"execution_count":null}]}